{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e987e2",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Forecasting: GARCH, LSTM, Transformer\n",
    "\n",
    "This notebook demonstrates a systematic workflow for multivariate time series forecasting:\n",
    "1. **Load Data**: Read `sample_multivar_timeseries.csv` with multiple features\n",
    "2. **Install Dependencies**: Ensure all required packages are available\n",
    "3. **Preprocessing**: Normalize, engineer features, and prepare sequences\n",
    "4. **Models**: GARCH (univariate volatility), LSTM (RNN), Transformer (attention-based)\n",
    "5. **Pipeline**: Create a unified training and evaluation pipeline\n",
    "6. **Comparison**: Compare RMSE, MAE, and visualize predictions\n",
    "\n",
    "**Target column**: `NIFTY` (close prices) — predicted using other columns as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install required packages if not available\n",
    "packages = {\n",
    "    'arch': 'arch',\n",
    "    'tensorflow': 'tensorflow',\n",
    "    'torch': 'torch',\n",
    "    'sklearn': 'scikit-learn',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib'\n",
    "}\n",
    "\n",
    "for import_name, pip_name in packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"✓ {pip_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
    "        print(f\"✓ {pip_name} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ff1b8",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the multivariate time series from CSV. The target column is `NIFTY` (close prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/sample_multivar_timeseries.csv', parse_dates=['date'] if 'date' in open('./data/sample_multivar_timeseries.csv').readline() else False)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nBasic statistics:\\n{df.describe()}\")\n",
    "\n",
    "# Identify target column\n",
    "TARGET_COLUMN = 'NIFTY'\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    print(f\"\\nWarning: '{TARGET_COLUMN}' not found. Available columns: {df.columns.tolist()}\")\n",
    "    # Try to find close price column\n",
    "    close_cols = [col for col in df.columns if 'close' in col.lower() or 'nifty' in col.lower()]\n",
    "    TARGET_COLUMN = close_cols[0] if close_cols else df.columns[-1]\n",
    "    print(f\"Using '{TARGET_COLUMN}' as target column\")\n",
    "\n",
    "print(f\"\\nTarget column: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53440c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target column\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Time series plot\n",
    "axes[0].plot(df.index, df[TARGET_COLUMN], label=TARGET_COLUMN, linewidth=2)\n",
    "axes[0].set_title(f'Time Series: {TARGET_COLUMN}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[1].hist(df[TARGET_COLUMN], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f'Distribution: {TARGET_COLUMN}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Price')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573c60a",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Handle missing values, create training/test splits, normalize, and prepare sequences for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_clean = df.fillna(method='ffill').fillna(method='bfill')\n",
    "print(f\"Missing values after filling: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Drop date column if present (for analysis)\n",
    "if 'date' in df_clean.columns:\n",
    "    df_clean = df_clean.drop('date', axis=1)\n",
    "\n",
    "# Select numerical columns only\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_clean = df_clean[numerical_cols]\n",
    "\n",
    "print(f\"Numerical features: {df_clean.shape[1]}\")\n",
    "print(f\"Dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Normalize using MinMaxScaler (for LSTM/Transformer)\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(df_clean),\n",
    "    columns=df_clean.columns\n",
    ")\n",
    "\n",
    "# Create train/test split (80/20)\n",
    "TRAIN_SIZE = 0.8\n",
    "split_idx = int(len(df_scaled) * TRAIN_SIZE)\n",
    "\n",
    "df_train = df_scaled[:split_idx]\n",
    "df_test = df_scaled[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "print(f\"Train/Test split: {TRAIN_SIZE*100:.0f}% / {(1-TRAIN_SIZE)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for deep learning (sliding window approach)\n",
    "def create_sequences(data, target_col_idx, window_size=30, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sequences for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        data: scaled DataFrame\n",
    "        target_col_idx: index of target column in data\n",
    "        window_size: number of timesteps to look back\n",
    "        forecast_horizon: number of steps ahead to predict\n",
    "    \n",
    "    Returns:\n",
    "        X, y: feature and target sequences\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - forecast_horizon + 1):\n",
    "        X.append(data.iloc[i:i+window_size].values)\n",
    "        y.append(data.iloc[i+window_size+forecast_horizon-1, target_col_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "WINDOW_SIZE = 30\n",
    "FORECAST_HORIZON = 1\n",
    "TARGET_COL_IDX = df_scaled.columns.get_loc(TARGET_COLUMN)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    df_train, TARGET_COL_IDX, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    df_test, TARGET_COL_IDX, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train_seq.shape} (samples, timesteps, features)\")\n",
    "print(f\"y_train shape: {y_train_seq.shape}\")\n",
    "print(f\"X_test shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test shape: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bde2d5",
   "metadata": {},
   "source": [
    "## 3. Model 1: GARCH (Univariate Volatility Model)\n",
    "\n",
    "GARCH models volatility of returns. We fit it on training returns and produce short-term variance forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7636140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Prepare GARCH: compute log returns from the original (unscaled) data\n",
    "original_prices = df_clean[TARGET_COLUMN].values\n",
    "returns = 100 * np.diff(np.log(original_prices))\n",
    "\n",
    "# Fit GARCH(1,1) on training returns\n",
    "train_returns = returns[:split_idx-1]\n",
    "test_returns = returns[split_idx-1:]\n",
    "\n",
    "print(f\"Training returns shape: {train_returns.shape}\")\n",
    "print(f\"Test returns shape: {test_returns.shape}\")\n",
    "\n",
    "# Fit GARCH model\n",
    "garch_model = arch_model(train_returns, vol='Garch', p=1, q=1, dist='normal')\n",
    "garch_result = garch_model.fit(disp='off')\n",
    "\n",
    "print(\"\\nGARCH Model Summary:\")\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Forecast conditional variance for test period\n",
    "garch_forecast = garch_result.forecast(horizon=len(test_returns), reindex=False)\n",
    "garch_conditional_var = garch_forecast.variance.values.flatten()\n",
    "\n",
    "print(f\"\\nGARCH variance forecast shape: {garch_conditional_var.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57642e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple price forecast from GARCH: use mean return + variance scaling\n",
    "# This is a simplified approach; in practice, combine with other models\n",
    "garch_price_pred = df_clean[TARGET_COLUMN].iloc[split_idx:].values.copy()\n",
    "for i in range(len(test_returns)):\n",
    "    # Naive: use previous price + scaled variance\n",
    "    mean_return = np.mean(train_returns)\n",
    "    scaled_return = mean_return + (np.sqrt(garch_conditional_var[i]) * 0.01)\n",
    "    garch_price_pred[i] = original_prices[split_idx + i - 1] * (1 + scaled_return / 100)\n",
    "\n",
    "# Inverse transform to original scale for comparison\n",
    "garch_price_pred_rescaled = garch_price_pred\n",
    "y_test_original = scaler_minmax.inverse_transform(\n",
    "    np.column_stack([df_test.iloc[WINDOW_SIZE:WINDOW_SIZE+len(y_test_seq), :].values])\n",
    ")[:, 0]\n",
    "\n",
    "print(f\"GARCH predictions (original scale): {garch_price_pred_rescaled[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ec117",
   "metadata": {},
   "source": [
    "## 4. Model 2: LSTM (Recurrent Neural Network)\n",
    "\n",
    "LSTM captures temporal dependencies and patterns in multivariate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd98efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(input_shape, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, activation='relu', return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Create and train LSTM\n",
    "print(\"Building LSTM model...\")\n",
    "lstm_model = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), lstm_units=64)\n",
    "lstm_model.summary()\n",
    "\n",
    "print(\"\\nTraining LSTM...\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"LSTM training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f528cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "print(\"LSTM predictions on test set...\")\n",
    "lstm_pred_scaled = lstm_model.predict(X_test_seq, verbose=0)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "# Create dummy array with right shape for inverse transform\n",
    "dummy = np.zeros((lstm_pred_scaled.shape[0], df_scaled.shape[1]))\n",
    "dummy[:, TARGET_COL_IDX] = lstm_pred_scaled.flatten()\n",
    "lstm_pred_original = scaler_minmax.inverse_transform(dummy)[:, TARGET_COL_IDX]\n",
    "\n",
    "# Get test targets in original scale\n",
    "dummy_test = np.zeros((y_test_seq.shape[0], df_scaled.shape[1]))\n",
    "dummy_test[:, TARGET_COL_IDX] = y_test_seq\n",
    "y_test_original = scaler_minmax.inverse_transform(dummy_test)[:, TARGET_COL_IDX]\n",
    "\n",
    "print(f\"LSTM predictions shape: {lstm_pred_original.shape}\")\n",
    "print(f\"Test targets shape: {y_test_original.shape}\")\n",
    "print(f\"First 5 predictions: {lstm_pred_original[:5]}\")\n",
    "print(f\"First 5 targets: {y_test_original[:5]}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].plot(lstm_history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('LSTM Training History - Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(lstm_history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(lstm_history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_title('LSTM Training History - MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c1ad7",
   "metadata": {},
   "source": [
    "## 5. Model 3: Transformer (Attention-Based)\n",
    "\n",
    "A simple Transformer-style model using MultiHeadAttention for capturing global patterns in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae877363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=32, num_heads=2, ff_dim=64, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Build a simple Transformer-style model for time series forecasting.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=head_size,\n",
    "        dropout=dropout_rate\n",
    "    )(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff_output = Dense(ff_dim, activation='relu')(x)\n",
    "    ff_output = Dropout(dropout_rate)(ff_output)\n",
    "    ff_output = Dense(input_shape[-1])(ff_output)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "    \n",
    "    # Global pooling and output\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Create and train Transformer\n",
    "print(\"Building Transformer model...\")\n",
    "transformer_model = build_transformer_model(\n",
    "    (X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "    head_size=16, num_heads=4, ff_dim=64\n",
    ")\n",
    "transformer_model.summary()\n",
    "\n",
    "print(\"\\nTraining Transformer...\")\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Transformer training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "print(\"Transformer predictions on test set...\")\n",
    "transformer_pred_scaled = transformer_model.predict(X_test_seq, verbose=0)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "dummy_trans = np.zeros((transformer_pred_scaled.shape[0], df_scaled.shape[1]))\n",
    "dummy_trans[:, TARGET_COL_IDX] = transformer_pred_scaled.flatten()\n",
    "transformer_pred_original = scaler_minmax.inverse_transform(dummy_trans)[:, TARGET_COL_IDX]\n",
    "\n",
    "print(f\"Transformer predictions shape: {transformer_pred_original.shape}\")\n",
    "print(f\"First 5 predictions: {transformer_pred_original[:5]}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].plot(transformer_history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(transformer_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('Transformer Training History - Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(transformer_history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(transformer_history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_title('Transformer Training History - MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624be5d",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare RMSE, MAE, and R² scores across all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Compute standard regression metrics.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n",
    "    \n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    \n",
    "    return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "# Compute metrics for all models\n",
    "results = []\n",
    "\n",
    "# GARCH (adjust length if needed)\n",
    "garch_pred_aligned = garch_price_pred_rescaled[:len(y_test_original)]\n",
    "results.append(compute_metrics(y_test_original, garch_pred_aligned, \"GARCH\"))\n",
    "\n",
    "# LSTM\n",
    "results.append(compute_metrics(y_test_original, lstm_pred_original, \"LSTM\"))\n",
    "\n",
    "# Transformer\n",
    "results.append(compute_metrics(y_test_original, transformer_pred_original, \"Transformer\"))\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c56a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "ax = axes[0, 0]\n",
    "ax.bar(comparison_df['Model'], comparison_df['RMSE'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax.set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(comparison_df['RMSE']):\n",
    "    ax.text(i, v + 0.1, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(comparison_df['Model'], comparison_df['MAE'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax.set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(comparison_df['MAE']):\n",
    "    ax.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# MAPE comparison\n",
    "ax = axes[1, 0]\n",
    "ax.bar(comparison_df['Model'], comparison_df['MAPE'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax.set_title('MAPE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('MAPE (%)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(comparison_df['MAPE']):\n",
    "    ax.text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# R² comparison\n",
    "ax = axes[1, 1]\n",
    "ax.bar(comparison_df['Model'], comparison_df['R2'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "ax.set_title('R² Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_ylim([min(comparison_df['R2']) - 0.1, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(comparison_df['R2']):\n",
    "    ax.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13334d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series prediction visualization\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot actual values\n",
    "ax.plot(range(len(y_test_original)), y_test_original, \n",
    "        'o-', label='Actual', linewidth=2, markersize=4, color='black', alpha=0.7)\n",
    "\n",
    "# Plot model predictions\n",
    "ax.plot(range(len(lstm_pred_original)), lstm_pred_original, \n",
    "        's--', label='LSTM', linewidth=1.5, markersize=3, alpha=0.7)\n",
    "ax.plot(range(len(transformer_pred_original)), transformer_pred_original, \n",
    "        '^--', label='Transformer', linewidth=1.5, markersize=3, alpha=0.7)\n",
    "ax.plot(range(len(garch_pred_aligned)), garch_pred_aligned, \n",
    "        '*--', label='GARCH', linewidth=1.5, markersize=6, alpha=0.7)\n",
    "\n",
    "ax.set_title(f'{TARGET_COLUMN}: Predictions vs Actual (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Price')\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945a314",
   "metadata": {},
   "source": [
    "## 7. Residual Analysis\n",
    "\n",
    "Analyze prediction errors to understand model strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals_lstm = y_test_original - lstm_pred_original\n",
    "residuals_transformer = y_test_original - transformer_pred_original\n",
    "residuals_garch = y_test_original - garch_pred_aligned\n",
    "\n",
    "# Plot residuals\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 10))\n",
    "\n",
    "# LSTM residuals\n",
    "axes[0, 0].plot(residuals_lstm, 'o-', alpha=0.7, color='#ff7f0e')\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_title('LSTM: Residuals over time', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residual')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(residuals_lstm, bins=30, edgecolor='black', alpha=0.7, color='#ff7f0e')\n",
    "axes[0, 1].set_title('LSTM: Residuals Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Residual')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Transformer residuals\n",
    "axes[1, 0].plot(residuals_transformer, 'o-', alpha=0.7, color='#2ca02c')\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "axes[1, 0].set_title('Transformer: Residuals over time', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Residual')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(residuals_transformer, bins=30, edgecolor='black', alpha=0.7, color='#2ca02c')\n",
    "axes[1, 1].set_title('Transformer: Residuals Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# GARCH residuals\n",
    "axes[2, 0].plot(residuals_garch, 'o-', alpha=0.7, color='#1f77b4')\n",
    "axes[2, 0].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "axes[2, 0].set_title('GARCH: Residuals over time', fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Residual')\n",
    "axes[2, 0].set_xlabel('Time Step')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2, 1].hist(residuals_garch, bins=30, edgecolor='black', alpha=0.7, color='#1f77b4')\n",
    "axes[2, 1].set_title('GARCH: Residuals Distribution', fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Residual')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "axes[2, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print residual statistics\n",
    "print(\"Residual Statistics\")\n",
    "print(\"=\"*70)\n",
    "print(f\"LSTM - Mean: {np.mean(residuals_lstm):.6f}, Std: {np.std(residuals_lstm):.6f}\")\n",
    "print(f\"Transformer - Mean: {np.mean(residuals_transformer):.6f}, Std: {np.std(residuals_transformer):.6f}\")\n",
    "print(f\"GARCH - Mean: {np.mean(residuals_garch):.6f}, Std: {np.std(residuals_garch):.6f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bb4f1",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "**Key Findings:**\n",
    "- LSTM and Transformer models leverage multivariate features for better predictions.\n",
    "- GARCH models volatility rather than price levels; it complements deep learning models.\n",
    "- The best model depends on data characteristics, target use case, and available computational resources.\n",
    "\n",
    "**Next Steps (for production):**\n",
    "1. **Hyperparameter tuning**: Use GridSearch or Optuna for optimal parameters.\n",
    "2. **Ensemble methods**: Combine predictions from multiple models.\n",
    "3. **Longer horizons**: Test multi-step-ahead forecasting.\n",
    "4. **Feature engineering**: Add technical indicators (RSI, MACD, Bollinger Bands, etc.).\n",
    "5. **External features**: Incorporate macroeconomic or sector-specific data.\n",
    "6. **Cross-validation**: Use time-series-aware CV (e.g., walk-forward validation).\n",
    "7. **Model deployment**: Serve models via FastAPI or Docker.\n",
    "\n",
    "**References:**\n",
    "- [Transformer for Time Series](https://keras.io/examples/timeseries/timeseries_transformer_classification/)\n",
    "- [GARCH Models](https://arch.readthedocs.io/)\n",
    "- [LSTM for Time Series](https://keras.io/examples/timeseries/lstm_seq2seq/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
